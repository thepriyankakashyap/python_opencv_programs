{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(window_name, canvas):\n",
    "    cv2.imshow(window_name, canvas)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyWindow(window_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining colors\n",
    "white = (255,255,255)\n",
    "black = (0,0,0)\n",
    "green = (0,255,0)\n",
    "blue = (255, 0,0)\n",
    "red = (0,0,255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions with OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary> ⭐⭐⭐⭐ OpenCV Image Convolutions (CLICK) </summary>\n",
    "\n",
    "\n",
    "<p>\n",
    "In reality, an (image) convolution is simply an element-wise multiplication of two matrices followed by a sum. <br>\n",
    "    1 . Take two matrices (which both have the same dimensions). <br>\n",
    "    2 . Multiply them, element-by-element (i.e., not the dot-product, just a simple multiplication). <br>\n",
    "    3 . Sum the elements together. <br> <br>\n",
    "    Think of an image as a big matrix and kernel or convolutional matrix as a tiny matrix that is used for blurring, sharpening, edge detection, and other image processing functions. This tiny kernel sits on top of the big image and slides from left-to-right and top-to-bottom, applying a mathematical operation (i.e., a convolution) at each (x, y)-coordinate of the original image. <br> <br>\n",
    "    Kernels can be an arbitrary size of M x N pixels, provided that both M and N are odd integers. <br>\n",
    "    1. Select an (x, y)-coordinate from the original image. <br>\n",
    "    2. Place the center of the kernel at this (x, y)-coordinate. <br>\n",
    "    3. Take the element-wise multiplication of the input image region and the kernel, then sum up the values of these multiplication operations into a single value. The sum of these multiplications is called the kernel output. <br>\n",
    "    4. Use the same (x, y)-coordinates from Step #1, but this time, store the kernel output in the same (x, y)-location as the output image. <br>\n",
    "</p>\n",
    "  \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage.exposure import rescale_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(image, kernel):\n",
    "    # grab the spatial dimensions of the image, along with\n",
    "    # the spatial dimensions of the kernel\n",
    "    (iH, iW) = image.shape[:2]\n",
    "    (kH, kW) = kernel.shape[:2]\n",
    "    \n",
    "    # allocate memory for the output image, taking care to\n",
    "    # \"pad\" the borders of the input image so the spatial\n",
    "    # size (i.e., width and height) are not reduced\n",
    "    pad = (kW - 1) // 2\n",
    "    \n",
    "    image = cv2.copyMakeBorder(image, pad, pad, pad, pad, cv2.BORDER_REPLICATE)\n",
    "    # cv2.BORDER_REPLICATE: It replicates the last element. Suppose, if image contains \n",
    "    # letters “abcdefgh” then output will be “aaaaa|abcdefgh|hhhhh“. \n",
    "    \n",
    "    output = np.zeros((iH, iW), dtype=\"float32\")\n",
    "    \n",
    "    # loop over the input image, \"sliding\" the kernel across\n",
    "    # each (x, y)-coordinate from left-to-right and top to\n",
    "    # bottom\n",
    "    for y in np.arange(pad, iH + pad):\n",
    "        for x in np.arange(pad, iW + pad):\n",
    "            # extract the ROI of the image by extracting the\n",
    "            # *center* region of the current (x, y)-coordinates dimensions\n",
    "            roi = image[y - pad:y + pad + 1, x - pad:x + pad + 1]\n",
    "            # perform the actual convolution by taking the\n",
    "            # element-wise multiplicate between the ROI and\n",
    "            # the kernel, then summing the matrix\n",
    "            k = (roi * kernel).sum()\n",
    "            # store the convolved value in the output (x,y)-\n",
    "            # coordinate of the output image\n",
    "            output[y - pad, x - pad] = k\n",
    "    \n",
    "    # rescale the output image to be in the range [0, 255]\n",
    "    output = rescale_intensity(output, in_range=(0, 255))\n",
    "    output = (output * 255).astype(\"uint8\")\n",
    "    # return the output image\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallBlur = np.ones((7, 7), dtype=\"float\") * (1.0 / (7 * 7))\n",
    "largeBlur = np.ones((21, 21), dtype=\"float\") * (1.0 / (21 * 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a sharpening filter\n",
    "sharpen = np.array(([0, -1, 0], [-1, 5, -1], [0, -1, 0]), dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the Laplacian kernel used to detect edge-like regions of an image\n",
    "laplacian = np.array(( [0, 1, 0], [1, -4, 1], [0, 1, 0]), dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the Sobel x-axis kernel\n",
    "sobelX = np.array(( [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]), dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the Sobel y-axis kernel\n",
    "sobelY = np.array(( [-1, -2, -1], [0, 0, 0], [1, 2, 1]), dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the kernel bank, a list of kernels we're going\n",
    "# to apply using both our custom `convole` function and\n",
    "# OpenCV's `filter2D` function\n",
    "kernelBank = (\n",
    "    (\"small_blur\", smallBlur),\n",
    "    (\"large_blur\", largeBlur),\n",
    "    (\"sharpen\", sharpen),\n",
    "    (\"laplacian\", laplacian),\n",
    "    (\"sobel_x\", sobelX),\n",
    "    (\"sobel_y\", sobelY)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] applying small_blur kernel\n",
      "[INFO] applying large_blur kernel\n",
      "[INFO] applying sharpen kernel\n",
      "[INFO] applying laplacian kernel\n",
      "[INFO] applying sobel_x kernel\n",
      "[INFO] applying sobel_y kernel\n"
     ]
    }
   ],
   "source": [
    "# load the input image and convert it to grayscale\n",
    "image = cv2.imread(\"batman.jpg\", 1)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# loop over the kernels\n",
    "for (kernelName, kernel) in kernelBank:\n",
    "    # apply the kernel to the grayscale image using both our custom `convole` function and OpenCV's `filter2D` function\n",
    "    \n",
    "    print(\"[INFO] applying {} kernel\".format(kernelName))\n",
    "    convoleOutput = convolve(gray, kernel)\n",
    "    opencvOutput = cv2.filter2D(gray, -1, kernel)\n",
    "    \n",
    "    # show the output images\n",
    "    cv2.imshow(\"original\", gray)\n",
    "    cv2.imshow(\"{} - convole\".format(kernelName), convoleOutput)\n",
    "    cv2.imshow(\"{} - opencv\".format(kernelName), opencvOutput)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Morphological Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary> ⭐⭐⭐⭐ OpenCV Morphological Operations (CLICK) </summary>\n",
    "\n",
    "\n",
    "<p>\n",
    "    Morphological operations are simple transformations applied to binary or grayscale images. \n",
    "    We can use morphological operations to increase the size of objects in images as well as decrease them. We can \n",
    "    also utilize morphological operations to close gaps between objects as well as open them. Like in image kernels, the structuring element slides from left-to-right and top-to-bottom for each pixel in the image. \n",
    "    \n",
    "    Morphological operations “probe” an image with a structuring element. Like in image kernels, the structuring element slides from left-to-right and top-to-bottom for each pixel in the image. \n",
    "</p>\n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = cv2.imread(\"my_logo.jpg\", 1)\n",
    "gray = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)\n",
    "display(\"gray\", gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erosion\n",
    "Erosion works by defining a structuring element and then sliding this structuring element from left-to-right and top-to-bottom across the input image. A foreground pixel in the input image will be kept only if all pixels inside the structuring element are > 0. Otherwise, the pixels are set to 0 (i.e., background).\n",
    "\n",
    "The second argument to cv2.erode is the structuring element. If this value is None, then a 3×3 structuring element, identical to the 8-neighborhood structuring element we saw above will be used. Of course, you could supply your own custom structuring element here instead of None as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eroded = cv2.erode(gray.copy(), None, )\n",
    "display(\"eroded\", eroded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 3):\n",
    "    eroded = cv2.erode(gray.copy(), None, iterations=i + 1)\n",
    "    display(\"eroded\", eroded)\n",
    "    \n",
    "# The for loop controls the number of times, or iterations, we are going to apply the erosion. As the number of \n",
    "# erosions increases, the foreground logo will start to “erode” and disappear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n",
    "Dilations increase the size of foreground objects and are especially useful for joining broken parts of an image together. Dilations, just as an erosion, also utilize structuring elements — a center pixel p of the structuring element is set to white if ANY pixel in the structuring element is > 0.\n",
    "\n",
    "Unlike an erosion where the foreground region is slowly eaten away at, a dilation actually grows our foreground region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilated = cv2.dilate(gray.copy(), None,)\n",
    "display(\"dilated\", dilated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    dilated = cv2.dilate(gray.copy(), None, iterations=i+1)\n",
    "    display(\"dilated\", dilated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening\n",
    "\n",
    "An opening is an erosion followed by a dilation.\n",
    "\n",
    "Performing an opening operation allows us to remove small blobs from an image: first an erosion is applied to remove the small blobs, then a dilation is applied to regrow the size of the original object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo1 = cv2.imread(\"my_logo1.jpg\", 1)\n",
    "logo1 = cv2.cvtColor(logo1, cv2.COLOR_BGR2GRAY)\n",
    "display(\"logo1\", logo1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelsizes = [(5,5), (7,7), (13,13)]\n",
    "# initialize a list of kernels sizes that will be applied to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernelsize in kernelsizes:\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, kernelsize)    \n",
    "    # cv2.MORPH_RECT, cv2.MORPH_ELLIPSE, cv2.MORPH_CROSS\n",
    "    # The cv2.getStructuringElement function requires two arguments: the first is the type of structuring element we \n",
    "    # want, and the second is the size of the structuring element\n",
    "    \n",
    "    opening = cv2.morphologyEx(logo1.copy(), cv2.MORPH_OPEN, kernel)\n",
    "    # The first required argument of cv2.morphologyEx is the image we want to apply the morphological operation to. \n",
    "    # The second argument is the actual type of morphological operation — in this case, it’s an opening operation. \n",
    "    # The last required argument is the kernel/structuring element that we are using.\n",
    "    \n",
    "    display(\"opening {}, {}\".format(kernelsize[0], kernelsize[1]), opening)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing\n",
    "The exact opposite to an opening would be a closing. A closing is a dilation followed by an erosion.\n",
    "\n",
    "As the name suggests, a closing is used to close holes inside of objects or for connecting components together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo2 = cv2.imread(\"my_logo2.jpg\",1)\n",
    "logo2 = cv2.cvtColor(logo2, cv2.COLOR_BGR2GRAY)\n",
    "display(\"logo2\", logo2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelsizes= [(3,3), (9,9), (13,13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernelsize in kernelsizes:\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelsize)\n",
    "    closing = cv2.morphologyEx(logo2.copy(), cv2.MORPH_CLOSE, kernel)\n",
    "    display(\"closing {}, {}\".format(kernelsize[0], kernelsize[1]), closing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Gradient\n",
    "\n",
    "A morphological gradient is the difference between a dilation and erosion. It is useful for determining the outline of a particular object of an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo3  = cv2.imread(\"my_logo2.jpg\",1)\n",
    "logo3 = cv2.cvtColor(logo3, cv2.COLOR_BGR2GRAY)\n",
    "display(\"logo3\", logo3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelsizes = [(3,3), (5,5), (7,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernelsize in kernelsizes:\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelsize)\n",
    "    \n",
    "    gradient = cv2.morphologyEx(logo3.copy(), cv2.MORPH_GRADIENT, kernel)\n",
    "    display(\"gradient {}, {}\".format(kernelsize[0], kernelsize[1],), gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Hat/White Hat\n",
    "A top hat (also known as a white hat) morphological operation is the difference between the original (grayscale/single channel) input image and the opening.\n",
    "\n",
    "A top hat operation is used to reveal bright regions of an image on dark backgrounds.\n",
    "\n",
    "Up until this point we have only applied morphological operations to binary images. But we can also apply morphological operations to grayscale images as well. In fact, both the top hat/white hat and the black hat operators are more suited for grayscale images rather than binary ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "car = cv2.imread(\"car.jpg\", 1)\n",
    "gray = cv2.cvtColor(car, cv2.COLOR_BGR2GRAY)\n",
    "display(\"car\", gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelSize = (13,5)\n",
    "# we define a rectangular structuring element with a width of 13 pixels and a height of 5 pixels. Structuring \n",
    "# elements can be of arbitrary size. And in this case, we are applying a rectangular element that is almost 3x wider \n",
    "# than it is tall. Because a license plate is roughly 3x wider than it is tall!\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelSize)\n",
    "\n",
    "tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, kernel)\n",
    "display(\"tophat\", tophat)\n",
    "\n",
    "blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)\n",
    "display(\"blackhat\", blackhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV Smoothing and Blurring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary> ⭐⭐⭐⭐ OpenCV Smoothing & Blurring (CLICK) </summary>\n",
    "\n",
    "\n",
    "<p>\n",
    "    Practically, this means that each pixel in the image is mixed in with its surrounding pixel intensities. This “mixture” of pixels in a neighborhood becomes our blurred pixel.\n",
    "    \n",
    "By smoothing an image prior to applying techniques such as edge detection or thresholding we are able to reduce the amount of high-frequency content, such as noise and edges (i.e., the “detail” of an image).\n",
    "</p>\n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average blurring ( cv2.blur )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python blurring.py\n",
    "\n",
    "# import the necessary packages\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "# load the image, display it to our screen, and initialize a list of\n",
    "# kernel sizes (so we can evaluate the relationship between kernel\n",
    "# size and amount of blurring)\n",
    "image = cv2.imread(\"batman.jpg\",1)\n",
    "cv2.imshow(\"Original\", image)\n",
    "kernelSizes = [(3, 3), (9, 9), (15, 15)]\n",
    "\n",
    "# loop over the kernel sizes\n",
    "for (kX, kY) in kernelSizes:\n",
    "\t# apply an \"average\" blur to the image using the current kernel\n",
    "\t# size\n",
    "\tblurred = cv2.blur(image, (kX, kY))\n",
    "\tcv2.imshow(\"Average ({}, {})\".format(kX, kY), blurred)\n",
    "\tcv2.waitKey(0)\n",
    "\n",
    "# close all windows to cleanup the screen\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imshow(\"Original\", image)\n",
    "\n",
    "# loop over the kernel sizes again\n",
    "for (kX, kY) in kernelSizes:\n",
    "\t# apply a \"Gaussian\" blur to the image\n",
    "\tblurred = cv2.GaussianBlur(image, (kX, kY), 0)\n",
    "\tcv2.imshow(\"Gaussian ({}, {})\".format(kX, kY), blurred)\n",
    "\tcv2.waitKey(0)\n",
    "\n",
    "# close all windows to cleanup the screen\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imshow(\"Original\", image)\n",
    "\n",
    "# loop over the kernel sizes a final time\n",
    "for k in (3, 9, 15):\n",
    "\t# apply a \"median\" blur to the image\n",
    "\tblurred = cv2.medianBlur(image, k)\n",
    "\tcv2.imshow(\"Median {}\".format(k), blurred)\n",
    "\tcv2.waitKey(0)\n",
    "\tcv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python bilateral.py\n",
    "\n",
    "# import the necessary packages\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--image\", type=str, default=\"adrian.png\",\n",
    "\thelp=\"path to input image\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# load the image, display it to our screen, and construct a list of\n",
    "# bilateral filtering parameters that we are going to explore\n",
    "image = cv2.imread(args[\"image\"])\n",
    "cv2.imshow(\"Original\", image)\n",
    "params = [(11, 21, 7), (11, 41, 21), (11, 61, 39)]\n",
    "\n",
    "# loop over the diameter, sigma color, and sigma space\n",
    "for (diameter, sigmaColor, sigmaSpace) in params:\n",
    "\t# apply bilateral filtering to the image using the current set of\n",
    "\t# parameters\n",
    "\tblurred = cv2.bilateralFilter(image, diameter, sigmaColor, sigmaSpace)\n",
    "\n",
    "\t# show the output image and associated parameters\n",
    "\ttitle = \"Blurred d={}, sc={}, ss={}\".format(\n",
    "\t\tdiameter, sigmaColor, sigmaSpace)\n",
    "\tcv2.imshow(title, blurred)\n",
    "\tcv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
